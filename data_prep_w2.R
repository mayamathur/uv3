
# Contact: Maya Mathur (mmathur@stanford.edu)


############################### SET UP ###############################

# load raw, wide-format Qualtrics data
setwd(data.dir)
library(readr)
d = read_csv(w2.data.name)

library(testthat)
# from Qualtrics (the +2 is for extra header rows)
expect_equal(nrow(d), 434 + 2)

# remove rows with NA site
d = d[ !is.na(d$w2_site), ]

# remove training trials
d = d[ , -grep( "train", names(d) ) ]

# number of real, non-training faces
if ( is.real.data == FALSE ) n.stim = 6
if ( is.real.data == TRUE ) n.stim = 183

# stimulus names
stim.names = paste( "face.", 1:n.stim, sep="" )

# manipulations needed only for pilot data
if ( is.real.data == FALSE ) {
  # pretend there were only 6 faces because otherwise there are NAs and it becomes confused
  # grab columns with radio button decisions for each LM stimulus in nonrandom order
  # the "_" is because Qualtrics names the variables "1_cat", etc.
  cols = grep( "_cat", names(d) )
  cols2 = cols[1:n.stim]
  drop = cols[ !cols %in% cols2 ]
  d = d[ , -drop ]
  # this means the pilot data are basically fake
  
  keep = d$w2_site %in% pilot.site
  keep[1:2] = TRUE
  
  # remove messed-up pilots from Ithaca
  if ( pilot.site == "Ithaca College" ) {
    keepers = paste( "P", as.character(100:109), sep = "" )
    d = d[ d$w2_id1 %in% keepers, ]
    
    # remove P104, which had no data at all
    d = d[ as.character(d$w2_id1) != "P104", ]
  }
}

# keep data only from the sites that finished data collection
# need to keep the extra header rows that will be used in reordering the
#  the random vectors
if ( is.real.data == TRUE ) {
  keep = d$w2_site %in% valid.sites
}

# temporarily pull out the header rows (to be added back later since needed for making URL key)
headers = d[1:2,]

d = d[ keep, ]
nrow(d)  # number of subjects (not header rows)


############################### HANDLE IDIOSYNCRASIES ###############################

# make unique subject-site ID
d$w2_uID = paste( d$w2_site, d$w2_id1, sep=" ") 

d = fix_site_idiosyncrasies( dat = d,
                             wave.num = 2 )

# only 1 fake row after running the above
nrow(d) 


############################### MAKE STIMULUS-URL KEY FOR RANDOMIZED LOOP & MERGE ###############################

# put back the header rows for making URL key
headers$w2_uID = NA
temp = rbind( as.data.frame(headers),
              as.data.frame(d) )

make_url_key( dat = temp,
             n.stim = n.stim,
             stim.names = stim.names,
            lm.varname = "cat", 
            key.dir = key.dir )

# read in the key 
setwd(key.dir)
( key = read.csv("autogenerated_stimulus_vs_url_key.csv") )



############################### REMOVE BAD DATA ###############################

# important: this needs to be created as a global variable with this name
# because the subsequent functions will populate it with subjects that should be excluded
# and the column names can't be changed
exclusions = data.frame( w2_uID = NA, reason = NA )


# this requires actually trying to make the subject lists for 1 coordinate variable
#  in order to check for problems

# # turn out to be troublesome in the dry run below
# # removing them now because they cause the function to stop
# # they will be automatically listed in the exclusions below, though
# exclude.ids.2 = c("University of Pennsylvania 43344",  # continuous timing stopped
#                   "Occidental College BL04", # everything is NA
#                   "Occidental College LB16")  
# d = d[ !d$w2_uID %in% exclude.ids.2, ]
# 
# # split_on_face( data = d,
# #                var.name = "xPos",
# #                id = 412,
# #                reorder = TRUE,
# #                key = key )


# dry run to see which subjects need to be excluded
# for pilots only, using reorder = FALSE because we're only testing on a subset of the URLs
invisible( get_subject_lists( data = d, 
                        var.name = "xPos",
                        reorder = is.real.data,
                        key = key,
                        rescale = TRUE ) )


# see how many need to be excluded for each reason
table(exclusions$reason)
length(unique(exclusions$w2_uID))  

# which sites had exclusions?
setwd(results.dir)
write.csv( table( d$w2_site[ d$w2_uID %in% exclusions$w2_uID ] ),
           "subjects_excluded_by_site.csv", 
           row.names = FALSE )

# remove them, but not the ones with nonstandard pixel dimensions
excl2 = exclusions[ !exclusions$reason == "Nonstandard pixel dimensions.", ]
d = d[ !d$w2_uID %in% excl2$w2_uID, ]

nrow(d)


############################### WHAT ALERTS DID SUBJECTS GET? ###############################

# won't work for pilots because of NAs
if ( is.real.data == TRUE ) {
  alerts = describe_alerts( dat = d,
                            n.stim = n.stim,
                            key = key )
}


############################### PARSE CURSOR DATA ###############################

# lists for each of the three spacetime variables
# these will be ordered correctly, not randomized

# so the [[i]][[j]] entry of each of these is the ith subject's vector
#  of values for stimulus j (where j is the order in the key, not the randomized order
#  it was shown)

# rescale so that every face trajectory has length 1 in x and y direction

# x-coordinates
xl = get_subject_lists( data = d, 
                        var.name = "xPos",
                        reorder = is.real.data,  
                        key = key,
                        rescale = TRUE )
# there are still warnings about nonstandard pixel dimensions, but that
#  is expected because we're not removing them

# y-coordinates
yl = get_subject_lists( data = d, 
                        var.name = "yPos",
                        reorder = is.real.data,
                        key = key,
                        rescale = TRUE )

# time coordinates
tl = get_subject_lists( data = d, 
                        var.name = "time",
                        reorder = is.real.data,
                        key = key )


# add outcome variables to wide-format data
d = add_outcomes( d, xl, yl, tl )

# exclude the one subject with a single negative reaction time
# I confirmed via their lists that data actually look like that
d = d[ -352, ]


# look for missing data on mediators
is.med.cols = grepl( "area", names(d) )
has.nas = apply( d[ , is.med.cols ],
       1,
       function(row) any(is.na(row)))
View(d[ has.nas,])


# save R objects
setwd(data.dir)
save( xl, file = "xl_subject_lists.RData" )
save( yl, file = "yl_subject_lists.RData" )
save( tl, file = "tl_subject_lists.RData" )



####################### NUISANCE COVARIATES THAT WILL BE ADJUSTED IN ANALYSIS #######################

# commented part: sanity check
# # look at raw x-distances
# xl.raw = get_subject_lists( data = d, 
#                             var.name = "xPos",
#                             reorder = is.real.data, 
#                             key = key,
#                             rescale = FALSE )
# 
# yl.raw = get_subject_lists( data = d, 
#                             var.name = "yPos",
#                             reorder = is.real.data, 
#                             key = key,
#                             rescale = FALSE )
# 
# # x-distances traveled from initial to final cursor position
# xdl = lapply( xl.raw, FUN = function(subj.list)
#   lapply( subj.list, FUN = function(face.list)
#     abs( face.list[[length(face.list)]] - face.list[[1]] )
#   )
# ) 
# 
# x.dists = unlist(xdl)
# summary(x.dists)
# 
# # how often are the x-distances off?
# # hard-coded because these are the standard pixel dimensions of experiment
# min.x = 235 - 10
# max.x = 335 + 10
# prop.table( table( x.dists < min.x ) )
# prop.table( table( x.dists > max.x ) )
# 
# # sanity check
# library(testthat)
# # a single face list
# temp = xl.raw[[1]][[1]]
# expect_equal( abs( temp[length(temp)] - temp[1] ), xdl[[1]][[1]] )


# identify subjects who ever had window too small
rows = grep( "4", d$alerts )
# make indicator variable for having window too small
d$wts = FALSE
d$wts[ rows ] = TRUE

# identify subjects who ever had pixel scaling issues
weird.scalers = exclusions$w2_uID[ exclusions$reason == "Nonstandard pixel dimensions." ]
d$weird.scaling = FALSE
d$weird.scaling[ d$w2_uID %in% weird.scalers ] = TRUE 

table(d$wts)
table(d$weird.scaling)


# save final wide dataset 
# ~~ note that this is prior to removing face 156 because its cursor trajectories are 
#  embedded in the xPos, yPos, t, etc.
setwd(data.dir)
write.csv( d, "wave2_wide_data_prepped.csv", row.names = FALSE )



####################### RESHAPE INTO LONG FORM #######################

# for analysis, we want a long-form dataset with 1 row per trial
# each row represents one unique stimulus and subject combo

l = wide_to_long( dat = d, 
                  stim.names = stim.names,
                  id.name = "id1" )

expect_equal( n.stim * nrow(d), nrow(l) )


############################### REMOVE ACCIDENTAL DUPLICATE FACE ###############################

# remove face 156 (accidental duplicate image)
l = l[ l$stim.name != "face.156", ]




setwd(data.dir)
write.csv(l, "w2_long_data_prepped.csv")

# # write all other R objects as well
# setwd(results.dir)
# save.image( file = "all_data_prep_objects.RData")


