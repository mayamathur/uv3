
# Contact: Maya Mathur (mmathur@stanford.edu)


############################### SET UP ###############################

# load raw, wide-format Qualtrics data
setwd(data.dir)
library(readr)
d = read_csv(w2.data.name)

library(testthat)
# from Qualtrics (the +2 is for extra header rows)
expect_equal(nrow(d), 434 + 2)

# remove rows with NA site
d = d[ !is.na(d$w2_site), ]


# remove training trials
d = d[ , -grep( "train", names(d) ) ]

# number of real, non-training faces
if ( is.real.data == FALSE ) n.stim = 6
if ( is.real.data == TRUE ) n.stim = 183

# stimulus names
stim.names = paste( "face.", 1:n.stim, sep="" )


# only for pilot data
if ( is.real.data == FALSE ) {
  # pretend there were only 6 faces because otherwise there are NAs and it becomes confused
  # grab columns with radio button decisions for each LM stimulus in nonrandom order
  # the "_" is because Qualtrics names the variables "1_cat", etc.
  cols = grep( "_cat", names(d) )
  cols2 = cols[1:n.stim]
  drop = cols[ !cols %in% cols2 ]
  d = d[ , -drop ]
  # this means the data are basically fake
}

# also need to keep the extra header rows that will be used in reordering the
#  the random vectors
if ( is.real.data == TRUE ) {
  keep = d$w2_site %in% valid.sites
  keep[1:2] = TRUE  
}

if ( is.real.data == FALSE ) {
  keep = d$w2_site %in% pilot.site
  keep[1:2] = TRUE
  
  # remove messed-up pilots from Ithaca
  if ( pilot.site == "Ithaca College" ) {
    keepers = paste( "P", as.character(100:109), sep = "" )
    d = d[ d$w2_id1 %in% keepers, ]
    
    # remove P104, which had no data at all
    d = d[ as.character(d$w2_id1) != "P104", ]
  }
}

d = d[ keep, ]
nrow(d) - 2  # number of subjects (not header rows)


############################### HANDLE IDIOSYNCRASIES ###############################

# make unique subject-site ID
d$w2_uID = paste( d$w2_site, d$w2_id1, sep=" ") 


##### Politecnico di Milano #####

# two subjects had same ID for the first wave
# fix the second one
d$w2_uID[ d$w2_site == "Politecnico di Milano" ]
d$w2_uID[ d$w2_uID == "Politecnico di Milano RVU" & d$RecordedDate == "2019-02-21 03:52:35" ] = "Politecnico di Milano OWH"


##### Eotvos Lorand #####
d$w2_uID[ d$w2_site == "Eotvos Lorand University" ]

# one subject entered univerisity ID instead of experiment ID
d$w2_uID[ d$w2_uID == "Eotvos Lorand University JXX096"] = "Eotvos Lorand University 24"

##### Exclude Subjects With Waves Run Out of Order #####
# they should only be in wave 2 for Eotvos Lorand
exclude.ids = c( paste( "Eotvos Lorand University", 31:38, sep = " " ),
                 "Politecnico di Milano XAQ" )

d = d[ !d$w2_uID %in% exclude.ids, ]

nrow(d) - 2  # don't count header rows


############################### LOOK FOR SUBJECTS WITH MISMATCHED IDS ###############################

# make unique subject-site ID
d$w2_uID = paste( d$w2_site, d$w2_id1, sep=" ")  

# find subjects with mismatched IDs
uID2 = paste( d$w2_site, d$w2_id2, sep=" ")
doesnt.match = which( as.character(d$w2_uID) != as.character(uID2) )
( doesnt.match = doesnt.match[-c(1:2)]  ) # remove the header rows

d$w2_uID[ doesnt.match ]


##### Exclude Them #####
# important: this needs to be created as a global variable with this name
# because the subsequent functions will populate it with subjects that should be excluded
# and the column names can't be changed
exclusions = data.frame( w2_uID = d$w2_uID[doesnt.match],
                                             reason = rep( "Mismatched IDs", length(doesnt.match) ) )
d = d[ ! d$w2_uID %in% exclusions$w2_uID, ]


############################### MAKE STIMULUS-URL KEY FOR RANDOMIZED LOOP & MERGE ###############################

make_url_key( dat = d,
             n.stim = n.stim,
             stim.names = stim.names,
            lm.varname = "cat", 
            key.dir = key.dir )

# read in the key 
setwd(key.dir)
key = read.csv("autogenerated_stimulus_vs_url_key.csv")

# remove extra header rows that we used for making key above
d = d[3:nrow(d),]


# sample size
( initial.n = nrow(d) )



############################### REMOVE BAD DATA ###############################

# this requires actually trying to make the subject lists for 1 coordinate variable
#  in order to check for problems

# dry run to see which subjects need to be excluded
# for pilots only, using reorder = FALSE because we're only testing on a subset of the URLs
invisible( get_subject_lists( data = d, 
                        var.name = "xPos",
                        reorder = is.real.data,
                        key = key,
                        rescale = TRUE ) )

# see how many need to be excluded for each reason
table(exclusions$reason)
length(unique(exclusions$w2_uID))  

# which sites had exclusions?
table( d$w2_site[ d$w2_uID %in% exclusions$w2_uID ] )

# remove them, but not the ones with nonstandard pixel dimensions
excl2 = exclusions[ !exclusions$reason == "Nonstandard pixel dimensions.", ]
d = d[ !d$w2_uID %in% excl2$w2_uID, ]

nrow(d)


############################### WHAT ALERTS DID SUBJECTS GET? ###############################

# won't work for pilots because of NAs
if ( is.real.data == TRUE ) {
  alerts = describe_alerts( dat = d,
                            n.stim = n.stim,
                            key = key )
}


############################### PARSE CURSOR DATA ###############################

# lists for each of the three spacetime variables
# these will be ordered correctly, not randomized

# so the [[i]][[j]] entry of each of these is the ith subject's vector
#  of values for stimulus j (where j is the order in the key, not the randomized order
#  it was shown)

# rescale so that every face trajectory has length 1 in x and y direction

# x-coordinates
xl = get_subject_lists( data = d, 
                        var.name = "xPos",
                        reorder = is.real.data,  
                        key = key,
                        rescale = TRUE )
# there are still warnings about nonstandard pixel dimensions, but that
#  is expected because we're not removing them

# y-coordinates
yl = get_subject_lists( data = d, 
                        var.name = "yPos",
                        reorder = is.real.data,
                        key = key,
                        rescale = TRUE )

# time coordinates
tl = get_subject_lists( data = d, 
                        var.name = "time",
                        reorder = is.real.data,
                        key = key )

# add outcome variables to wide-format data
d = add_outcomes( d, xl, yl, tl )


# save R objects
setwd(data.dir)
save( xl, file = "xl_subject_lists.RData" )
save( yl, file = "yl_subject_lists.RData" )
save( tl, file = "tl_subject_lists.RData" )



####################### NUISANCE COVARIATES THAT WILL BE ADJUSTED IN ANALYSIS #######################

# look at raw x-distances
xl.raw = get_subject_lists( data = d, 
                            var.name = "xPos",
                            reorder = is.real.data, 
                            key = key,
                            rescale = FALSE )

yl.raw = get_subject_lists( data = d, 
                            var.name = "yPos",
                            reorder = is.real.data, 
                            key = key,
                            rescale = FALSE )

# x-distances traveled from initial to final cursor position
xdl = lapply( xl.raw, FUN = function(subj.list)
  lapply( subj.list, FUN = function(face.list)
    abs( face.list[[length(face.list)]] - face.list[[1]] )
  )
) 

x.dists = unlist(xdl)
summary(x.dists)

# how often are the x-distances off?
# hard-coded because these are the standard pixel dimensions of experiment
min.x = 235 - 10
max.x = 335 + 10
prop.table( table( x.dists < min.x ) )
prop.table( table( x.dists > max.x ) )

# sanity check
library(testthat)
# a single face list
temp = xl.raw[[1]][[1]]
expect_equal( abs( temp[length(temp)] - temp[1] ), xdl[[1]][[1]] )


# identify subjects who ever had window too small
rows = grep( "4", d$alerts )
# make indicator variable for having window too small
d$wts = FALSE
d$wts[ rows ] = TRUE

# identify subjects who ever had pixel scaling issues
weird.scalers = exclusions$w2_uID[ exclusions$reason == "Nonstandard pixel dimensions." ]
d$weird.scaling = FALSE
d$weird.scaling[ d$w2_uID %in% weird.scalers ] = TRUE 

table(d$wts)
table(d$weird.scaling)


# save final wide dataset 
setwd(data.dir)
write.csv( d, "wave2_wide_data_prepped.csv", row.names = FALSE )



####################### RESHAPE INTO LONG FORM #######################

# for analysis, we want a long-form dataset with 1 row per trial
# each row represents one unique stimulus and subject combo

l = wide_to_long( dat = d, 
                  stim.names = stim.names,
                  id.name = "id1" )

expect_equal( n.stim * nrow(d), nrow(l) )


setwd(data.dir)
write.csv(l, "w2_long_data_prepped.csv")

# write all other R objects as well
setwd(results.dir)
save.image( file = "all_data_prep_objects.RData")


